from __future__ import annotations

from typing import Dict, Any, Tuple, List
import cv2
import numpy as np
from PIL import Image
import io
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import logging
from scipy import ndimage
from scipy.stats import entropy
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class EnhancedDeepfakeDetector:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # Initialize analysis parameters
        self.analysis_weights = {
            'frequency_analysis': 0.25,
            'texture_analysis': 0.20,
            'face_consistency': 0.20,
            'color_analysis': 0.15,
            'edge_analysis': 0.10,
            'compression_analysis': 0.10
        }
        
        logger.info("âœ… Enhanced Deepfake detector initialized")

    def _analyze_frequency_domain(self, image: np.ndarray) -> Dict[str, float]:
        """Analyze frequency domain characteristics"""
        try:
            # Convert to grayscale
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # Apply FFT
            f_transform = np.fft.fft2(gray)
            f_shift = np.fft.fftshift(f_transform)
            magnitude_spectrum = np.log(np.abs(f_shift) + 1)
            
            # Analyze frequency patterns
            center_y, center_x = magnitude_spectrum.shape[0] // 2, magnitude_spectrum.shape[1] // 2
            
            # High frequency content analysis
            high_freq_mask = np.zeros_like(magnitude_spectrum)
            high_freq_mask[center_y-50:center_y+50, center_x-50:center_x+50] = 1
            high_freq_mask = 1 - high_freq_mask
            
            high_freq_energy = np.sum(magnitude_spectrum * high_freq_mask)
            total_energy = np.sum(magnitude_spectrum)
            high_freq_ratio = high_freq_energy / (total_energy + 1e-10)
            
            # Frequency variance analysis
            freq_variance = np.var(magnitude_spectrum)
            
            # Detect unnatural frequency patterns (common in AI-generated content)
            freq_anomaly_score = 0
            if high_freq_ratio < 0.15:  # Too little high frequency content
                freq_anomaly_score += 0.3
            if freq_variance > 15:  # Unusual frequency variance
                freq_anomaly_score += 0.4
            if freq_variance < 5:  # Too uniform frequency distribution
                freq_anomaly_score += 0.3
            
            return {
                'high_freq_ratio': high_freq_ratio,
                'freq_variance': freq_variance,
                'anomaly_score': min(freq_anomaly_score, 1.0),
                'is_suspicious': freq_anomaly_score > 0.5
            }
            
        except Exception as e:
            logger.error(f"Frequency analysis error: {e}")
            return {'anomaly_score': 0.0, 'is_suspicious': False}

    def _analyze_texture_patterns(self, image: np.ndarray) -> Dict[str, float]:
        """Analyze texture patterns for AI generation artifacts"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # Local Binary Pattern analysis
            lbp = self._calculate_lbp(gray)
            lbp_variance = np.var(lbp)
            lbp_entropy = entropy(lbp.flatten() + 1e-10)
            
            # Gabor filter analysis
            gabor_responses = []
            for angle in [0, 45, 90, 135]:
                kernel = cv2.getGaborKernel((21, 21), 5, np.radians(angle), 10, 0.5, 0, ktype=cv2.CV_32F)
                response = cv2.filter2D(gray, cv2.CV_8UC3, kernel)
                gabor_responses.append(np.mean(response))
            
            gabor_variance = np.var(gabor_responses)
            
            # Texture anomaly detection
            texture_anomaly_score = 0
            if lbp_variance < 50:  # Too uniform texture
                texture_anomaly_score += 0.3
            if lbp_entropy < 6:  # Low texture complexity
                texture_anomaly_score += 0.3
            if gabor_variance < 100:  # Unusual Gabor response patterns
                texture_anomaly_score += 0.4
            
            return {
                'lbp_variance': lbp_variance,
                'lbp_entropy': lbp_entropy,
                'gabor_variance': gabor_variance,
                'anomaly_score': min(texture_anomaly_score, 1.0),
                'is_suspicious': texture_anomaly_score > 0.5
            }
            
        except Exception as e:
            logger.error(f"Texture analysis error: {e}")
            return {'anomaly_score': 0.0, 'is_suspicious': False}

    def _analyze_face_consistency(self, image: np.ndarray) -> Dict[str, float]:
        """Analyze face consistency and naturalness"""
        try:
            # Face detection
            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            faces = face_cascade.detectMultiScale(gray, 1.1, 4)
            
            if len(faces) == 0:
                return {'anomaly_score': 0.0, 'is_suspicious': False, 'face_count': 0}
            
            face_anomaly_score = 0
            face_analysis = []
            
            for (x, y, w, h) in faces:
                # Extract face region
                face_roi = gray[y:y+h, x:x+w]
                
                # Analyze face symmetry
                left_half = face_roi[:, :w//2]
                right_half = cv2.flip(face_roi[:, w//2:], 1)
                
                # Resize to match dimensions
                min_width = min(left_half.shape[1], right_half.shape[1])
                left_half = left_half[:, :min_width]
                right_half = right_half[:, :min_width]
                
                symmetry_diff = np.mean(np.abs(left_half.astype(float) - right_half.astype(float)))
                
                # Analyze facial features consistency
                face_edges = cv2.Canny(face_roi, 50, 150)
                edge_density = np.sum(face_edges > 0) / (face_edges.shape[0] * face_edges.shape[1])
                
                # Check for unnatural patterns
                if symmetry_diff > 30:  # Unnatural asymmetry
                    face_anomaly_score += 0.3
                if edge_density > 0.2:  # Too many edges (artifacts)
                    face_anomaly_score += 0.4
                if edge_density < 0.05:  # Too few edges (over-smoothed)
                    face_anomaly_score += 0.3
                
                face_analysis.append({
                    'symmetry_diff': symmetry_diff,
                    'edge_density': edge_density
                })
            
            avg_anomaly_score = face_anomaly_score / len(faces) if faces is not None else 0
            
            return {
                'anomaly_score': min(avg_anomaly_score, 1.0),
                'is_suspicious': avg_anomaly_score > 0.5,
                'face_count': len(faces),
                'face_analysis': face_analysis
            }
            
        except Exception as e:
            logger.error(f"Face analysis error: {e}")
            return {'anomaly_score': 0.0, 'is_suspicious': False, 'face_count': 0}

    def _analyze_color_distribution(self, image: np.ndarray) -> Dict[str, float]:
        """Analyze color distribution patterns"""
        try:
            # Convert to different color spaces
            hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
            lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
            
            # Analyze color channel distributions
            color_anomaly_score = 0
            
            # HSV analysis
            h_channel = hsv[:, :, 0]
            s_channel = hsv[:, :, 1]
            v_channel = hsv[:, :, 2]
            
            h_variance = np.var(h_channel)
            s_variance = np.var(s_channel)
            v_variance = np.var(v_channel)
            
            # Check for unnatural color patterns
            if h_variance < 100:  # Too uniform hue
                color_anomaly_score += 0.2
            if s_variance < 50:  # Too uniform saturation
                color_anomaly_score += 0.2
            if v_variance < 200:  # Too uniform brightness
                color_anomaly_score += 0.2
            
            # LAB color space analysis
            l_channel = lab[:, :, 0]
            a_channel = lab[:, :, 1]
            b_channel = lab[:, :, 2]
            
            lab_variance = np.var(a_channel) + np.var(b_channel)
            if lab_variance < 100:  # Unnatural LAB distribution
                color_anomaly_score += 0.4
            
            return {
                'hsv_variance': h_variance + s_variance + v_variance,
                'lab_variance': lab_variance,
                'anomaly_score': min(color_anomaly_score, 1.0),
                'is_suspicious': color_anomaly_score > 0.5
            }
            
        except Exception as e:
            logger.error(f"Color analysis error: {e}")
            return {'anomaly_score': 0.0, 'is_suspicious': False}

    def _analyze_edge_patterns(self, image: np.ndarray) -> Dict[str, float]:
        """Analyze edge patterns for AI generation artifacts"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # Multiple edge detection methods
            edges_canny = cv2.Canny(gray, 50, 150)
            edges_sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
            edges_sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
            edges_sobel = np.sqrt(edges_sobel_x**2 + edges_sobel_y**2)
            
            # Analyze edge characteristics
            canny_density = np.sum(edges_canny > 0) / (edges_canny.shape[0] * edges_canny.shape[1])
            sobel_variance = np.var(edges_sobel)
            
            # Edge anomaly detection
            edge_anomaly_score = 0
            if canny_density > 0.15:  # Too many edges (artifacts)
                edge_anomaly_score += 0.4
            if canny_density < 0.03:  # Too few edges (over-smoothed)
                edge_anomaly_score += 0.3
            if sobel_variance > 1000:  # Unusual edge variance
                edge_anomaly_score += 0.3
            
            return {
                'canny_density': canny_density,
                'sobel_variance': sobel_variance,
                'anomaly_score': min(edge_anomaly_score, 1.0),
                'is_suspicious': edge_anomaly_score > 0.5
            }
            
        except Exception as e:
            logger.error(f"Edge analysis error: {e}")
            return {'anomaly_score': 0.0, 'is_suspicious': False}

    def _analyze_compression_artifacts(self, image: np.ndarray) -> Dict[str, float]:
        """Analyze compression artifacts that might indicate AI generation"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # DCT analysis (similar to JPEG compression analysis)
            # Resize to 8x8 blocks for DCT
            h, w = gray.shape
            h8 = (h // 8) * 8
            w8 = (w // 8) * 8
            gray_resized = cv2.resize(gray, (w8, h8))
            
            # Apply DCT to 8x8 blocks
            dct_coeffs = []
            for i in range(0, h8-7, 8):
                for j in range(0, w8-7, 8):
                    block = gray_resized[i:i+8, j:j+8].astype(np.float32)
                    dct_block = cv2.dct(block)
                    dct_coeffs.append(dct_block.flatten())
            
            dct_coeffs = np.array(dct_coeffs)
            
            # Analyze DCT coefficient patterns
            high_freq_coeffs = dct_coeffs[:, 1:]  # Exclude DC component
            high_freq_variance = np.var(high_freq_coeffs)
            high_freq_mean = np.mean(np.abs(high_freq_coeffs))
            
            # Compression artifact detection
            compression_anomaly_score = 0
            if high_freq_variance < 10:  # Too uniform high frequency content
                compression_anomaly_score += 0.4
            if high_freq_mean < 5:  # Unusually low high frequency energy
                compression_anomaly_score += 0.3
            if high_freq_mean > 50:  # Unusually high high frequency energy
                compression_anomaly_score += 0.3
            
            return {
                'high_freq_variance': high_freq_variance,
                'high_freq_mean': high_freq_mean,
                'anomaly_score': min(compression_anomaly_score, 1.0),
                'is_suspicious': compression_anomaly_score > 0.5
            }
            
        except Exception as e:
            logger.error(f"Compression analysis error: {e}")
            return {'anomaly_score': 0.0, 'is_suspicious': False}

    def _calculate_lbp(self, image: np.ndarray) -> np.ndarray:
        """Calculate Local Binary Pattern"""
        try:
            rows, cols = image.shape
            lbp = np.zeros_like(image)
            
            for i in range(1, rows - 1):
                for j in range(1, cols - 1):
                    center = image[i, j]
                    binary_string = ""
                    
                    # 8-neighborhood
                    neighbors = [
                        image[i-1, j-1], image[i-1, j], image[i-1, j+1],
                        image[i, j+1], image[i+1, j+1], image[i+1, j],
                        image[i+1, j-1], image[i, j-1]
                    ]
                    
                    for neighbor in neighbors:
                        binary_string += "1" if neighbor >= center else "0"
                    
                    lbp[i, j] = int(binary_string, 2)
            
            return lbp
        except:
            return np.zeros_like(image)

    def analyze_media(self, file_data: bytes, content_type: str) -> Dict[str, Any]:
        """Enhanced media analysis with multiple detection methods"""
        try:
            if content_type.startswith('image/'):
                return self._analyze_image(file_data)
            elif content_type.startswith('video/'):
                return self._analyze_video(file_data)
            else:
                return {
                    "label": "Unsupported",
                    "confidence": 0,
                    "trust_score": 0,
                    "reason_analysis": "Unsupported file type. Please upload an image or video file.",
                    "model": "unsupported"
                }
        except Exception as e:
            logger.error(f"Media analysis error: {e}")
            return {
                "label": "Error",
                "confidence": 0,
                "trust_score": 0,
                "reason_analysis": f"Error analyzing media: {str(e)}",
                "model": "error"
            }

    def _analyze_image(self, image_data: bytes) -> Dict[str, Any]:
        """Analyze a single image with enhanced methods"""
        try:
            # Convert bytes to numpy array
            image = Image.open(io.BytesIO(image_data))
            image_array = np.array(image)
            
            # Perform comprehensive analysis
            frequency_analysis = self._analyze_frequency_domain(image_array)
            texture_analysis = self._analyze_texture_patterns(image_array)
            face_analysis = self._analyze_face_consistency(image_array)
            color_analysis = self._analyze_color_distribution(image_array)
            edge_analysis = self._analyze_edge_patterns(image_array)
            compression_analysis = self._analyze_compression_artifacts(image_array)
            
            # Calculate weighted anomaly score
            total_anomaly_score = (
                frequency_analysis['anomaly_score'] * self.analysis_weights['frequency_analysis'] +
                texture_analysis['anomaly_score'] * self.analysis_weights['texture_analysis'] +
                face_analysis['anomaly_score'] * self.analysis_weights['face_consistency'] +
                color_analysis['anomaly_score'] * self.analysis_weights['color_analysis'] +
                edge_analysis['anomaly_score'] * self.analysis_weights['edge_analysis'] +
                compression_analysis['anomaly_score'] * self.analysis_weights['compression_analysis']
            )
            
            # Determine label and confidence
            is_deepfake = total_anomaly_score > 0.4  # Threshold for deepfake detection
            confidence = int(total_anomaly_score * 100)
            
            if is_deepfake:
                label = "AI-Generated"
                confidence = max(confidence, 60)  # Minimum confidence for positive detection
            else:
                label = "Human-Created"
                confidence = max(100 - confidence, 60)  # Minimum confidence for negative detection
            
            # Generate detailed analysis
            suspicious_regions = []
            if frequency_analysis['is_suspicious']:
                suspicious_regions.append('Unnatural frequency patterns')
            if texture_analysis['is_suspicious']:
                suspicious_regions.append('Artificial texture patterns')
            if face_analysis['is_suspicious']:
                suspicious_regions.append('Face consistency issues')
            if color_analysis['is_suspicious']:
                suspicious_regions.append('Unnatural color distribution')
            if edge_analysis['is_suspicious']:
                suspicious_regions.append('Edge pattern anomalies')
            if compression_analysis['is_suspicious']:
                suspicious_regions.append('Compression artifacts')
            
            # Generate reason analysis
            if is_deepfake:
                reason_analysis = f"High confidence AI-generated detection ({confidence}%). "
                if suspicious_regions:
                    reason_analysis += f"Detected: {', '.join(suspicious_regions[:3])}. "
                reason_analysis += "This media shows characteristics commonly associated with AI-generated content."
            else:
                reason_analysis = f"Media appears to be human-created ({confidence}% confidence). "
                if suspicious_regions:
                    reason_analysis += f"Minor anomalies detected: {', '.join(suspicious_regions[:2])}. "
                reason_analysis += "No significant signs of AI generation detected."
            
            return {
                "label": label,
                "confidence": confidence,
                "trust_score": confidence,
                "reason_analysis": reason_analysis,
                "raw_score": total_anomaly_score,
                "model": "enhanced-deepfake-detector",
                "file_type": "image",
                "is_deepfake": is_deepfake,
                "details": {
                    "suspicious_regions": suspicious_regions,
                    "model_used": ["Frequency Analysis", "Texture Analysis", "Face Consistency", "Color Analysis", "Edge Analysis", "Compression Analysis"],
                    "analysis_time": 0.5,
                    "face_count": face_analysis.get('face_count', 0),
                    "anomaly_scores": {
                        "frequency": frequency_analysis['anomaly_score'],
                        "texture": texture_analysis['anomaly_score'],
                        "face": face_analysis['anomaly_score'],
                        "color": color_analysis['anomaly_score'],
                        "edge": edge_analysis['anomaly_score'],
                        "compression": compression_analysis['anomaly_score']
                    }
                }
            }
            
        except Exception as e:
            logger.error(f"Image analysis error: {e}")
            return {
                "label": "Error",
                "confidence": 0,
                "trust_score": 0,
                "reason_analysis": f"Error processing image: {str(e)}",
                "model": "error"
            }

    def _analyze_video(self, video_data: bytes) -> Dict[str, Any]:
        """Analyze video with frame sampling"""
        try:
            # Extract frames from video
            frames = self._extract_frames_from_video(video_data)
            
            if not frames:
                return {
                    "label": "Error",
                    "confidence": 0,
                    "trust_score": 0,
                    "reason_analysis": "Could not extract frames from video",
                    "model": "error"
                }
            
            # Analyze each frame
            frame_results = []
            for frame in frames:
                frame_analysis = self._analyze_image_features(frame)
                frame_results.append(frame_analysis)
            
            # Aggregate results with temporal consistency analysis
            avg_anomaly_score = np.mean([r["anomaly_score"] for r in frame_results])
            temporal_consistency = self._analyze_temporal_consistency(frame_results)
            
            # Adjust score based on temporal consistency
            final_anomaly_score = avg_anomaly_score * (1 + temporal_consistency)
            final_anomaly_score = min(final_anomaly_score, 1.0)
            
            # Determine label and confidence
            is_deepfake = final_anomaly_score > 0.4
            confidence = int(final_anomaly_score * 100)
            
            if is_deepfake:
                label = "AI-Generated"
                confidence = max(confidence, 60)
            else:
                label = "Human-Created"
                confidence = max(100 - confidence, 60)
            
            # Generate reason analysis
            if is_deepfake:
                reason_analysis = f"High confidence AI-generated video detection ({confidence}%). "
                reason_analysis += f"Analyzed {len(frames)} frames with temporal consistency analysis. "
                reason_analysis += "This video shows characteristics commonly associated with AI-generated content."
            else:
                reason_analysis = f"Video appears to be human-created ({confidence}% confidence). "
                reason_analysis += f"Analyzed {len(frames)} frames with temporal consistency analysis. "
                reason_analysis += "No significant signs of AI generation detected."
            
            return {
                "label": label,
                "confidence": confidence,
                "trust_score": confidence,
                "reason_analysis": reason_analysis,
                "raw_score": final_anomaly_score,
                "model": "enhanced-deepfake-detector-video",
                "file_type": "video",
                "is_deepfake": is_deepfake,
                "details": {
                    "suspicious_regions": ["Temporal inconsistencies", "Frame-level anomalies"],
                    "model_used": ["Frame Analysis", "Temporal Consistency", "Multi-frame Detection"],
                    "analysis_time": 2.0,
                    "frames_analyzed": len(frames),
                    "temporal_consistency": temporal_consistency
                }
            }
            
        except Exception as e:
            logger.error(f"Video analysis error: {e}")
            return {
                "label": "Error",
                "confidence": 0,
                "trust_score": 0,
                "reason_analysis": f"Error processing video: {str(e)}",
                "model": "error"
            }

    def _extract_frames_from_video(self, video_data: bytes) -> List[np.ndarray]:
        """Extract frames from video data"""
        try:
            video_stream = io.BytesIO(video_data)
            cap = cv2.VideoCapture()
            cap.open(video_stream)
            
            frames = []
            frame_count = 0
            max_frames = 15  # Increased frame count for better analysis
            
            while frame_count < max_frames:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # Convert BGR to RGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(frame_rgb)
                frame_count += 1
            
            cap.release()
            return frames
            
        except Exception as e:
            logger.error(f"Error extracting video frames: {e}")
            return []

    def _analyze_image_features(self, image: np.ndarray) -> Dict[str, float]:
        """Analyze image features for deepfake detection"""
        try:
            # Perform comprehensive analysis
            frequency_analysis = self._analyze_frequency_domain(image)
            texture_analysis = self._analyze_texture_patterns(image)
            face_analysis = self._analyze_face_consistency(image)
            color_analysis = self._analyze_color_distribution(image)
            edge_analysis = self._analyze_edge_patterns(image)
            compression_analysis = self._analyze_compression_artifacts(image)
            
            # Calculate weighted anomaly score
            total_anomaly_score = (
                frequency_analysis['anomaly_score'] * self.analysis_weights['frequency_analysis'] +
                texture_analysis['anomaly_score'] * self.analysis_weights['texture_analysis'] +
                face_analysis['anomaly_score'] * self.analysis_weights['face_consistency'] +
                color_analysis['anomaly_score'] * self.analysis_weights['color_analysis'] +
                edge_analysis['anomaly_score'] * self.analysis_weights['edge_analysis'] +
                compression_analysis['anomaly_score'] * self.analysis_weights['compression_analysis']
            )
            
            return {
                "anomaly_score": total_anomaly_score,
                "frequency_score": frequency_analysis['anomaly_score'],
                "texture_score": texture_analysis['anomaly_score'],
                "face_score": face_analysis['anomaly_score'],
                "color_score": color_analysis['anomaly_score'],
                "edge_score": edge_analysis['anomaly_score'],
                "compression_score": compression_analysis['anomaly_score']
            }
            
        except Exception as e:
            logger.error(f"Error analyzing image features: {e}")
            return {"anomaly_score": 0.0}

    def _analyze_temporal_consistency(self, frame_results: List[Dict[str, float]]) -> float:
        """Analyze temporal consistency across video frames"""
        try:
            if len(frame_results) < 2:
                return 0.0
            
            # Calculate variance in anomaly scores across frames
            anomaly_scores = [r["anomaly_score"] for r in frame_results]
            temporal_variance = np.var(anomaly_scores)
            
            # High temporal variance suggests AI generation
            # (AI-generated videos often have inconsistent frame quality)
            temporal_consistency_score = min(temporal_variance * 0.1, 0.5)
            
            return temporal_consistency_score
            
        except Exception as e:
            logger.error(f"Temporal consistency analysis error: {e}")
            return 0.0


# Keep the old class for backward compatibility
class DeepfakeDetector(EnhancedDeepfakeDetector):
    """Backward compatibility wrapper"""
    pass
